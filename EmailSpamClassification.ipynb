{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE8260cHeCpz"
   },
   "source": [
    "# **Spam Classification**\n",
    "We have many e-mail service providers which provide an in-built spam filter to protect you from malicious e-mails. We are going to try and make our very own spam filters.\n",
    "\n",
    "We are going to train a Machine Learning Model which takes an e-mail (or rather the feature vector of an e-mail $x \\in \\mathbb{R}^n$ where $n$ is the dimension of the feature vector of the e-mail) and outputs $y \\in \\{0,1\\}$ where $0$ denotes **not-spam** (also called ham) and $1$ denotes **spam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT0tpLdBP8ae"
   },
   "source": [
    "# **1. Loading Data**\n",
    "We are going to use the open source dataset provided by the <a href=\"https://spamassassin.apache.org/old/publiccorpus/\">SpamAssassin Public Corpus</a> which is one of the most famous datasets available. It classifies e-mails as '**Spam**' or '**Ham**' (Non-Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjhVKARSO8A3"
   },
   "outputs": [],
   "source": [
    "HAM_URL = \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\"\n",
    "HAM_URL_2 = \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\"\n",
    "HARD_HAM_URL = \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\"\n",
    "SPAM_URL = \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\"\n",
    "SPAM_URL_2 = \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2\"\n",
    "url_list = [HAM_URL, HAM_URL_2, HARD_HAM_URL, SPAM_URL, SPAM_URL_2]\n",
    "DATASET_DIR = \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBp2k52tXnW6"
   },
   "source": [
    "## 1.1. Downloading Compressed E-Mail Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oltBK3VL8Wp",
    "outputId": "d098cfe9-2273-4bac-c365-fcc7becce191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-16 21:45:07--  https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1612216 (1.5M) [application/x-bzip2]\n",
      "Saving to: ‘20030228_easy_ham.tar.bz2’\n",
      "\n",
      "20030228_easy_ham.t 100%[===================>]   1.54M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2021-12-16 21:45:08 (41.1 MB/s) - ‘20030228_easy_ham.tar.bz2’ saved [1612216/1612216]\n",
      "\n",
      "--2021-12-16 21:45:08--  https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1077892 (1.0M) [application/x-bzip2]\n",
      "Saving to: ‘20030228_easy_ham_2.tar.bz2’\n",
      "\n",
      "20030228_easy_ham_2 100%[===================>]   1.03M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2021-12-16 21:45:09 (28.6 MB/s) - ‘20030228_easy_ham_2.tar.bz2’ saved [1077892/1077892]\n",
      "\n",
      "--2021-12-16 21:45:09--  https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1029898 (1006K) [application/x-bzip2]\n",
      "Saving to: ‘20030228_hard_ham.tar.bz2’\n",
      "\n",
      "20030228_hard_ham.t 100%[===================>]   1006K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-12-16 21:45:10 (32.1 MB/s) - ‘20030228_hard_ham.tar.bz2’ saved [1029898/1029898]\n",
      "\n",
      "--2021-12-16 21:45:10--  https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1183768 (1.1M) [application/x-bzip2]\n",
      "Saving to: ‘20030228_spam.tar.bz2’\n",
      "\n",
      "20030228_spam.tar.b 100%[===================>]   1.13M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-12-16 21:45:11 (33.8 MB/s) - ‘20030228_spam.tar.bz2’ saved [1183768/1183768]\n",
      "\n",
      "--2021-12-16 21:45:12--  https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2\n",
      "Resolving spamassassin.apache.org (spamassassin.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to spamassassin.apache.org (spamassassin.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2059563 (2.0M) [application/x-bzip2]\n",
      "Saving to: ‘20030228_spam_2.tar.bz2’\n",
      "\n",
      "20030228_spam_2.tar 100%[===================>]   1.96M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2021-12-16 21:45:13 (44.4 MB/s) - ‘20030228_spam_2.tar.bz2’ saved [2059563/2059563]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Command Line way to get the e-mail '.tar' files. \n",
    "# Now once we download the .tar files, we just need to unzip them.\n",
    "!wget {HAM_URL}\n",
    "\"\"\"Similarly write command line code to download the other URLs as well.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qi0s0eqeV5m4",
    "outputId": "5930c51a-0097-404a-8440-66db57872f2b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Printing the Current Working Directory (CWD)\n",
    "\"\"\"Write code here\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFXNGoEQXs_p"
   },
   "source": [
    "## 1.2. Extracting the E-Mails from Compressed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDHF3fZqMzDu",
    "outputId": "bc5cceb8-8f15-459c-95ff-5942c3d91c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20030228_easy_ham.tar.bz2', '20030228_easy_ham_2.tar.bz2', '20030228_hard_ham.tar.bz2', '20030228_spam.tar.bz2', '20030228_spam_2.tar.bz2']\n",
      "/content/dataset\n"
     ]
    }
   ],
   "source": [
    "# Unzipping all the '.tar' e-mail files\n",
    "\n",
    "def unzip_tar_files(tar_files):\n",
    "    # Function stores extracted emails to './content/dataset'\n",
    "    \"\"\"Write code here to get Current Working Directory,\n",
    "    store it in the variable cwd.\n",
    "    \n",
    "    Create a new directory path by joining DATASET_DIR and\n",
    "    cwd and store it in the variable new_dir. Print new_dir\n",
    "    \n",
    "    Use the os library to make the new_dir directory\n",
    "    \"\"\"\n",
    "    # Extracting .tar files for each easy_ham, easy_ham_2 etc.\n",
    "    for tar_file in tar_files:\n",
    "        with tarfile.open(tar_file, 'r:bz2') as f:\n",
    "            f.extractall(path=new_dir)\n",
    "\n",
    "tar_data_files = [url.split('/')[-1] for url in url_list]\n",
    "print(tar_data_files)\n",
    "unzip_tar_files(tar_data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xJBD5zsg0ma"
   },
   "source": [
    "## 1.3. Command Line Interface\n",
    "\n",
    "Instead of writing such a difficult to read code above, we could simply 'untar' our files using **Colab's Command Line Interface (CLI)**\n",
    " \n",
    "Using $!$ in front of a command in a Colab/Jupyter Cell runs the command as a Command Line command.\n",
    "\n",
    "```\n",
    "!tar xvjf 20030228_easy_ham.tar.bz2\n",
    "!tar xvjf 20030228_easy_ham_2.tar.bz2\n",
    "!tar xvjf 20030228_hard_ham.tar.bz2  \n",
    "!tar xvjf 20030228_spam.tar.bz2\n",
    "!tar xvjf 20050311_spam_2.tar.bz2\n",
    "```\n",
    "<br>\n",
    "\n",
    "The above is the same as writing\n",
    "```\n",
    "!tar xvjf {HAM_URL.split('/')[-1]}\n",
    "!tar xvjf {HAM_URL_2.split('/')[-1]}\n",
    "!tar xvjf {HARD_HAM_URL.split('/')[-1]}\n",
    "!tar xvjf {SPAM_URL.split('/')[-1]}\n",
    "!tar xvjf {SPAM_URL_2.split('/')[-1]}\n",
    "```\n",
    "<br>\n",
    "\n",
    "But now, the above Extracted Data Files are stored in folders in the Current Working Directory (CWD), thus we move these files to another directory\n",
    "```\n",
    "!mv easy_ham {DATASET_DIR}  \n",
    "!mv easy_ham_2 {DATASET_DIR}\n",
    "!mv hard_ham {DATASET_DIR}\n",
    "!mv spam {DATASET_DIR}\n",
    "!mv spam_2 {DATASET_DIR}\n",
    "```\n",
    "<br>\n",
    "\n",
    "Now for something even more amazing! The above two code chunks can be further compressed into just 5 lines of Command Line code.\n",
    "```\n",
    "!tar -C {DATASET_DIR} xvjf {HAM_URL.split('/')[-1]}\n",
    "!tar -C {DATASET_DIR} xvjf {HAM_URL_2.split('/')[-1]}\n",
    "!tar -C {DATASET_DIR} xvjf {HARD_HAM_URL.split('/')[-1]}\n",
    "!tar -C {DATASET_DIR} xvjf {SPAM_URL.split('/')[-1]}\n",
    "!tar -C {DATASET_DIR} xvjf {SPAM_URL_2.split('/')[-1]}\n",
    "```\n",
    "<br>\n",
    "\n",
    "My purpose of telling you all this is that in many cases you'll see software engineers use the **Command Line Interface** to do a lot of tasks instead of a **Graphical User Interface** which of course feels more intuitive (The classic old **CLI vs GUI debate**).\n",
    "\n",
    "As shown above, for general purpose use, of course GUIs dominate because they are much more intuitive (the Windows OS/ Mac OS UI is a GUI!) but for engineering problems, where we require efficiency and speed, knowledge of CLI absolutely demolishes GUI. This is one of the key takeaways from this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgLAWZ-dXx8L"
   },
   "source": [
    "## 1.4. Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRXrA1gGXep4"
   },
   "outputs": [],
   "source": [
    "# Creating our dataset\n",
    "def load_files(path):\n",
    "    file_names = os.listdir(path)\n",
    "    # List of strings containing the contents of the e-mails\n",
    "    emails_data = []\n",
    "\n",
    "    # Iterating through raw e-mails and converting them to string\n",
    "    for file_name in file_names:\n",
    "        with open(os.path.join(path, file_name), 'rb') as f:\n",
    "            raw_byte_data = f.read() \n",
    "            str_data = raw_byte_data.decode('utf-8', errors='ignore')\n",
    "            emails_data.append(str_data)\n",
    "\n",
    "    return emails_data\n",
    "\n",
    "easy_ham = load_files(f'/content/{DATASET_DIR}/easy_ham')\n",
    "easy_ham_2 = load_files(f'/content/{DATASET_DIR}/easy_ham_2')\n",
    "hard_ham = load_files(f'/content/{DATASET_DIR}/hard_ham')\n",
    "spam = load_files(f'/content/{DATASET_DIR}/spam')\n",
    "spam_2 = load_files(f'/content/{DATASET_DIR}/spam_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUUICgPacEez"
   },
   "outputs": [],
   "source": [
    "import sklearn.utils\n",
    "import numpy as np\n",
    "\n",
    "# I've left out hard_ham, you can choose to include it though\n",
    "\n",
    "X = easy_ham + easy_ham_2 + spam + spam_2 # + hard_ham\n",
    "\n",
    "# y denotes the label of the data - whether the mails are spam or ham (non-spam)\n",
    "# 1 - Spam , 0 - Ham (Non-Spam)\n",
    "y =  np.concatenate([np.zeros(len(easy_ham)), np.zeros(len(easy_ham_2)), \n",
    "                    np.ones(len(spam)), np.ones(len(spam_2))]) \n",
    "# y =  np.concatenate([np.zeros(len(easy_ham)), np.zeros(len(easy_ham_2)), \n",
    "#                    np.ones(len(spam)), np.ones(len(spam_2)), np.zeros(len(hard_ham))])\n",
    "\n",
    "# Shuffling our dataset\n",
    "\"\"\"Use the shuffle method in sklearn utils to shuffle X and y. Write Code here.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AIzDE9BNgPxb",
    "outputId": "a81a3a58-db82-4ece-8b43-756541334355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501 1401 251 501 1398\n"
     ]
    }
   ],
   "source": [
    "print(len(easy_ham),len(easy_ham_2),len(hard_ham),len(spam),len(spam_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30Bgj10lgIRx",
    "outputId": "b8419df8-9aac-434a-d3d4-289237f57f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5801 5801\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-DBAjmiQH3I"
   },
   "source": [
    "# **2. Data Pre-processing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmRU38VCikKJ"
   },
   "source": [
    "##2.1. Removing the E-Mail Header\n",
    "For the purpose of this project, we are only going to deal with the body of the e-mails and not any other kind of meta-data. That is, we are not going to use any information about the sender, sender's location, subject of the e-mail, sending time, IP Address etc. Of course, including this information would help us in creating better heuritstic rules and more accurate models, but they would complicate the feature extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9i3eoBEig7k"
   },
   "outputs": [],
   "source": [
    "def removeHeader(X, y):\n",
    "    X_body = []\n",
    "    y_body = []\n",
    "\n",
    "    for i, email in enumerate(X):\n",
    "        # If at all some e-mails are corrupted, hence need to use try-except\n",
    "        try:\n",
    "            header_start = email.find('\\n\\n')\n",
    "            email = email[header_start:]\n",
    "            X_body.append(email)\n",
    "            y_body.append(y[i])\n",
    "        except:\n",
    "            print(f\"Excluded Mail No. {i}\")\n",
    "\n",
    "    return X_body, np.array(y_body)\n",
    "\n",
    "X, y = removeHeader(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hu4jqcN8nN6G",
    "outputId": "06f4f412-71e2-46bd-ec7e-6962270a5e81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5801, 5801)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUKRtyTTnS7l"
   },
   "source": [
    "##2.2. Pre-processing E-Mail Body\n",
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset. The figure below shows a sample email that contains a URL, an email address (at the end), numbers, and dollar\n",
    "amounts.\n",
    "\n",
    "<img src=\"email.png\" width=\"700px\" />\n",
    "\n",
    "While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every\n",
    "email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the\n",
    "email with the unique string “httpaddr” to indicate that a URL was present.\n",
    "\n",
    "This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small. \n",
    "\n",
    "In the function `processEmail` below, we want to implement the following email preprocessing and normalization steps:\n",
    "\n",
    "- **Lower-casing**: The entire email is converted into lower case, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "\n",
    "- **Stripping HTML**: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "\n",
    "- **Normalizing URLs**: All URLs are replaced with the text “httpaddr”.\n",
    "\n",
    "- **Normalizing Email Addresses**:  All email addresses are replaced with the text “emailaddr”.\n",
    "\n",
    "- **Normalizing Numbers**: All numbers are replaced with the text “number”.\n",
    "\n",
    "- **Normalizing Dollars**: All dollar signs ($) are replaced with the text “dollar”.\n",
    "\n",
    "- **Word Stemming**: Words are reduced to their stemmed form. For example, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips off additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "\n",
    "- **Removal of non-words**: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n",
    "\n",
    "The result of these preprocessing steps is shown in the figure below. \n",
    "\n",
    "<img src=\"email_cleaned.png\" alt=\"email cleaned\" style=\"width: 600px;\"/>\n",
    "\n",
    "While preprocessing has left word fragments and non-words, this form turns out to be much easier to work with for performing feature extraction.\n",
    "\n",
    "**References**:\n",
    "1. Week 6 - Machine Learning System Design - Building a Spam Classifier <a href=\"https://www.coursera.org/learn/machine-learning\"> Machine Learning MOOC on Coursera by Andrew Ng</a>\n",
    "1. Programming Exercise 6 of <a href=\"https://www.coursera.org/learn/machine-learning\"> Machine Learning MOOC on Coursera by Andrew Ng</a>\n",
    "\n",
    "**Implementation Tip** : For many of the above mentioned tasks for data pre-processing, I (or rather Andrew Ng XD) have (has) used Regular Expressions (RegEx). While they can be really complicated to understand, they are relatively easy to learn and can be really powerful if used well. Additionally, many of the pre-processing steps we do here are fairly common and a really simple web search can give answers to most of these. But I would really recommend you to go through this **amazing short tutorial** about regular expressions.\n",
    "\n",
    "Link to the tutorial - https://regexone.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3faKKLtsQHjz"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Importing Regular Expressions (re) to handle the above mentioned pre-processing tasks\n",
    "# You can even choose to do it with Python's inbuilt String library \n",
    "import re\n",
    "import string\n",
    "\n",
    "def preprocessEmail(email_contents, verbose = False):\n",
    "    \"\"\"Preprocesses the body of an email and returns a list of indices \n",
    "    of the words contained in the email.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    email_contents : str\n",
    "        A string containing one email. \n",
    "    \n",
    "    verbose : bool\n",
    "        If True, print the resulting email after processing.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    processed_email : list\n",
    "        A list of strings/words containing the contents of a processed email.\n",
    "    \"\"\"\n",
    "    # ========================== Preprocess Email ===========================\n",
    "    # Turns all the elements to lower case\n",
    "    email_contents = \n",
    "    \n",
    "    # Strip all HTML Tags (Regex)\n",
    "    # Looks for any expression that starts with < and ends with > and does not \n",
    "    # have any < or > in the tag and replace it with a space\n",
    "    email_contents =re.compile('WriteRegexHere').sub('AppropriateStringHere', email_contents)\n",
    "\n",
    "    # Handle Numbers (Regex)\n",
    "    # Look for one or more characters between 0-9 and replace them with ' number '\n",
    "    email_contents = re.compile('WriteRegexHere').sub('AppropriateStringHere', email_contents)\n",
    "\n",
    "    # Handle URLS (Regex)\n",
    "    # Look for strings starting with http:// or https:// and replace the URL \n",
    "    # with ' httpaddr '\n",
    "    email_contents = re.compile('WriteRegexHere').sub('AppropriateStringHere', email_contents)\n",
    "\n",
    "    # Handle Email Addresses (Regex)\n",
    "    # Look for strings with @ in the middle, not surrounded by any whitespace &\n",
    "    # replace it with ' emailaddr '\n",
    "    email_contents = re.compile('WriteRegexHere').sub('AppropriateStringHere', email_contents)\n",
    "    \n",
    "    # Handle $ sign (Regex) and replace it with ' dollar '\n",
    "    email_contents = re.compile('WriteRegexHere').sub('AppropriateStringHere', email_contents)\n",
    "    \n",
    "    # Get rid of any punctuation\n",
    "    email_contents = email_contents.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove any empty word string\n",
    "    email_contents = \"\"\"Write Code Here\"\"\"\n",
    "    \n",
    "    # Stem the email contents word by word\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_email = []\n",
    "    for word in email_contents:\n",
    "        # Remove any remaining NON-alphanumeric characters in word and\n",
    "        # substitute it with \"\" (the null string)\n",
    "        word = re.compile('WriteRegexHere').sub('', word).strip()\n",
    "        \"\"\"Write code to stem the word and append it to processed_email list.\"\"\"\n",
    "\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "\n",
    "    if verbose:\n",
    "        print('----------------')\n",
    "        print('Processed email:')\n",
    "        print('----------------')\n",
    "        print(' '.join(processed_email))\n",
    "    return processed_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDcAvb_6wL68"
   },
   "outputs": [],
   "source": [
    "X_preprocessed = []\n",
    "\"\"\"\n",
    "Write code here to take all the mails from our dataset, process them and append\n",
    "them into an empty list X_preprocessed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzSYmwuvhBTR"
   },
   "source": [
    "\n",
    "## 2.3 Extracting Features from Emails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiFYB6BIw_OV"
   },
   "source": [
    "###2.3.1 Vocabulary Creation\n",
    "We haven't yet defined what our vocabulary is yet. Once we have pre-processed our emails, we select the **Top** $K$ highest frequency (highest occuring) words (which are stemmed) in our e-mail dataset to act as our vocabulary. That is we only consider these $K$ most frequently occuring words to describe our email contents / their feature vector. Here $K$ is a hyperparameter. A good starting point for $K$ can be around $1500$ to $2000$.\n",
    "\n",
    "**Very Important** : A lot of people make a mistake of NOT excluding **Stop Words** from their vocabulary. Stop Words are those words whose presence is gramatically important but NOT semantically. That is, these words do not add much to the context of a given document/ sentence. Eg. 'the', 'is', 'a', 'an', 'on', 'at', 'are' etc.\n",
    "\n",
    "There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. But there is general consensus on a lot of words falling into the category of Stop Words. Many NLP libraries have a list of their own stop words.\n",
    "\n",
    "**Implementation Tip** : Instead of executing this creation of vocabulary separately, we can save our time and modify the previous code cell in such a way that we can directly/ indirectly get a vocabulary list or atleast a frequency value for each encountered stemmed word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heLTGsfn2LUY"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "i = 0\n",
    "print(english_stopwords[i])\n",
    "print(len(english_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va8ZAR-qv_K6"
   },
   "outputs": [],
   "source": [
    "def createVocabulary(X_preprocessed):\n",
    "    vocabulary_list = []\n",
    "    \"\"\"Complete Code here to make your own vocabulary\"\"\"\n",
    "    return vocabulary_list\n",
    "\n",
    "vocabulary_list = createVocabulary(X_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPLrU4jyxF-F"
   },
   "source": [
    "###2.3.2. One Hot Encoding\n",
    "We will now implement a basic **One Hot Encoding** feature extraction that converts each email into a vector in $\\mathbb{R}^n$. For this exercise, you will be using $\\mbox{n = number of words in vocabulary list}$. Specifically, the feature $x_i \\in \\{0, 1\\}$ for an email corresponds to whether the $i^{th}$ word in the dictionary occurs in the email. That is, $x_i = 1$ if the $i^{th}$ word is in the email and $x_i = 0$ if the $i^{th}$ word is not present in the email.\n",
    "\n",
    "Thus, for a typical email, this feature would look like:\n",
    "\n",
    "$$ x = \\begin{bmatrix} \n",
    "0 & \\dots & 1 & 0 & \\dots & 1 & 0 & \\dots & 0 \n",
    "\\end{bmatrix}^T \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "You should now complete the code in the function `emailFeatures` to generate a feature vector for an email, given the `email_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_AfZfb7v_Xj"
   },
   "outputs": [],
   "source": [
    "def emailFeatures(email_content, vocabulary_list):\n",
    "    \"\"\"\n",
    "    Takes in a email_content vector and produces a feature vector from the word \n",
    "    indices. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    email_content : list\n",
    "        A list of stemmed words present in the processed e-mail.\n",
    "    \n",
    "    Returns\n",
    "    x : list \n",
    "        The computed feature vector.\n",
    "    \n",
    "    x[i] = 1 when word i\n",
    "    is present in the email. Concretely, if the word 'the' (say,\n",
    "    index 60) appears in the email, then x[60] = 1. The feature\n",
    "    vector should look like:\n",
    "        x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..]\n",
    "    \"\"\"\n",
    "    # Total number of words in the dictionary\n",
    "    n = len(vocabulary_list)\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    x = np.zeros(n)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "    \n",
    "    # ===========================================================\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkPe_3N65nkC"
   },
   "outputs": [],
   "source": [
    "X_processed = np.zeros((X.shape[0], len(vocabulary_list)))\n",
    "\"\"\"Write code to create X_processed - a NumPy array of size m*n where 'm' is the \n",
    "number of e-mails in our dataset and 'n' is the size of our vocabulary\n",
    "from X_preprocessed\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ou9li8H_U-k0"
   },
   "source": [
    "### 2.3.3. Other Feature Extraction Strategies\n",
    "The above One-Hot Encoding Feature Extraction strategy is nice, but is very simple and doesn't work well for other complicated datasets/ NLP tasks. For eg., it does not at all take into account the frequency of each vocabulary word in the mail or their relative positioning, or even their global occurrence in the entire e-mail corpus (in the train set.)\n",
    "\n",
    "\n",
    "Thus there have been many other proposals to use different kinds of word embeddings. Some of them are:\n",
    "1. Token Frequency (TF)\n",
    "1. Inverse Document Frequency (IDF)\n",
    "1. Word Embeddings (Bag of Words, GloVe, word2vec etc.)\n",
    "\n",
    "You could try these embeddings as an optional part of the project to see how the accuracy/ F1 Score of the ML model changes! Most of them have ready made implementations in the form of libraries\n",
    "\n",
    "**References**:\n",
    "1. https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08\n",
    "1. https://towardsdatascience.com/how-to-turn-text-into-features-478b57632e99\n",
    "1. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T8mfW9NQB4-"
   },
   "source": [
    "# **3. Exploratory Data Analysis & t-SNE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUL0wK_4BDBS"
   },
   "source": [
    "## 3.1. Data Analysis\n",
    "In general, the main purpose of any kind of data analysis is to gain insgihts about the data. Analyzing the data helps us identify the trends and structure in the data.\n",
    "\n",
    "In the case where we intend to use Machine Learning after our Data Analysis pipeline, the goal of our Data Analysis pipeline is not just figuring out the trends in the data, but also checking if the data satisfies the assumptions of our machine learning model.\n",
    "\n",
    "All machine learning models make a set of assumptions about the data (what they imagine the data would be like). These assumptions are called the [Inductive Bias](https://en.wikipedia.org/wiki/Inductive_bias) of a machine learning algorithm. If our data satisfies the inductive bias of our machine learning algorithm, even if our algorithm is simple (eg. Logistic Regression, k Nearest Neighbours etc.), it will give us much better results with lesser data even on comparison with complex models such as Neural Networks.\n",
    "\n",
    "A Data Science Pipeline can be split into 4 stages:\n",
    "1. **Exploratory Data Analysis** - Taking stock of our data, taking care of null values, understanding what all categories we have (eg. spam and non-spam), number of examples of each category, basic distribution of each feature, central-tendancy statistics of features such as mean, median, mode, variance etc.\n",
    "\n",
    "1. **Descriptive Data Analysis** - The part where we do a much more sophisticated analysis of the data - enough to draw good and insightful conclusions. Hypothesis Testing can also be done as a part of Descriptive Data Analysis, to test conclusions drawn from the data. \n",
    "\n",
    "1. **Predictive Analysis** - The Machine Learning part of the pipeline.\n",
    "\n",
    "1. **Prescriptive Analysis** - The part of the pipeline where we make suggestions to the client about actions to be taken in order to fulfill a certain objective by analyzing the data. It can be combined with/ drawn from the descriptive analysis done on the data.\n",
    "\n",
    "Here are a few links for ideas regarding Descriptive Analysis in NLP tasks:\n",
    "1. https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools\n",
    "1. https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_1PU2ZnP9Du"
   },
   "outputs": [],
   "source": [
    "\"\"\"Use as many code cells as required to perform the EDA\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glqG2_cb7UDW"
   },
   "source": [
    "##3.2. t-SNE Visualization\n",
    "t-distributed Stochastic Neighbor Embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a **two or three-dimensional map**. It is based on Stochastic Neighbor Embedding originally developed by Sam Roweis and Geoffrey Hinton, where Laurens van der Maaten proposed the t-distributed variant. It is a **nonlinear** dimensionality reduction technique well-suited for embedding high-dimensional data **for visualization** in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n",
    "\n",
    "Although t-SNE is a tool used in Descriptive Data Analysis, I put it up in a separate section since it is a Machine Learning algorithm and also, there are many similar algorithms which are used to create such visualizations. Eg. Maximum Variance Unfolding (MVU), Autoencoders, Locally-Linear Embedding (LLE) etc. Though a problem with many such algorithms such as MVU, LLE are that they don't work well on real life datasets (Autoencoders work really well though.)\n",
    "\n",
    "But a lot of people just consider t-SNE as a magic blackbox and interpret it in a grossly incorrect manner. [This website](https://distill.pub/2016/misread-tsne/) explains the mis-interpretations of t-SNE very well and how to correctly understand what the visualization means.\n",
    "\n",
    "**References** :\n",
    "1. Stochastic Neighbour Embedding Original Paper (SNE) - http://www.cs.toronto.edu/~fritz/absps/sne.pdf\n",
    "1. t-SNE Original Paper - https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf\n",
    "1. PCA vs t-SNE - https://stats.stackexchange.com/questions/238538/are-there-cases-where-pca-is-more-suitable-than-t-sne\n",
    "1. Lec 15: t-SNE by Prof Ali Ghodsi at UWaterloo - https://youtu.be/4GBgqmq0XAY?t=949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqySisKT7TxA"
   },
   "outputs": [],
   "source": [
    "\"\"\"We'll be using sklearn's implementation of t-SNE, Use as many code cells as necessary\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZL3XD99QPjL"
   },
   "source": [
    "# **4. Machine Learning & Hyperparameter Tuning**:\n",
    "We are going to train 4 Machine Learning classifiers and tune their hyperparameters to better fit their dataset. We are going to use the sklearn library to implement the algorithms.\n",
    "\n",
    "ML Algorithms : \n",
    "1. Logistic Regression\n",
    "1. Support Vector Machines\n",
    "1. Neural Network (Multi-Layer Perceptron)\n",
    "1. Random Forests Classifier\n",
    "\n",
    "Hyperparameters are those variables which are NOT learnt by the model during the training procedure. They are constants during training and define the 'architecture' of the model. Eg. Learning Rate, Number of Iterations in Gradient Descent, Kernel used in SVMs, Max Depth of Tree in Decision Trees, value of K in K-Nearest Neighbours etc. Thus it is essential to use the best hyperparameters to better suit our dataset.\n",
    "\n",
    "But of course, even before we begin implementing models, we would need to randomly divide our dataset into training, validation (for hyperparameter tuning) and test set.\n",
    "\n",
    "**Tutorials for Sklearn**:\n",
    "1. https://towardsdatascience.com/a-beginners-guide-to-scikit-learn-14b7e51d71a4\n",
    "1. https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "1. K-Fold Cross Validation - https://youtu.be/rjbkWSTjHzM?t=3848\n",
    "\n",
    "(We'll be using Stratified K-Fold Cross Validation for Hyperparameter Tuning instead of simple Cross Validation. Will use Sklearn library implementation)\n",
    "\n",
    "**References (Hyperparameter Tuning & ML Practice)**:\n",
    "1.  Week 6 - Evaluating a Learning Algorithm. [Machine Learning on Coursera by Andrew Ng ](https://www.coursera.org/learn/machine-learning)\n",
    "1. Week 6 - Handling Skewed Data. [Machine Learning on Coursera by Andrew Ng ](https://www.coursera.org/learn/machine-learning)\n",
    "1. Sections 3.2.1 & 3.2.2 https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y51xSCsVQPN9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use as many code cells required to implement hyperparameter tuning and fitting ML models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Use as many code cells required to implement hyperparameter tuning and fitting ML models\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "EmailSpamClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
